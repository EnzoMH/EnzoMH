<div align="center">

# ì‹ ëª…í˜¸ Shin Myeongho

**AI/ML Engineer** | LLM Fine-tuning & Production RAG

[![HuggingFace](https://img.shields.io/badge/ðŸ¤—_MyeongHo0621-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/MyeongHo0621)
[![HuggingFace](https://img.shields.io/badge/ðŸ¤—_soka0000-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/soka0000)
[![Email](https://img.shields.io/badge/isfs003@gmail.com-EA4335?style=flat-square&logo=gmail&logoColor=white)](mailto:isfs003@gmail.com)
[![Portfolio](https://img.shields.io/badge/Portfolio-000000?style=flat-square&logo=notion&logoColor=white)](https://www.notion.so/Shin-Myeong-Ho-32b17c808b3642a583ea457a0c68df5a)

</div>

---

## ðŸ‘‹ About Me

AI/ML Engineer focusing on **Korean LLM fine-tuning**, **RAG systems**, and **production-grade inference**.

- ðŸ§  Building my own **Korean LLM lineup** (7B / 14B / 3B) with Qwen2.5-based models
- ðŸ¤– Reduced RAG + LLM inference latency by **75%** (20s â†’ 5s) using vLLM & architecture tuning
- ðŸ­ Currently: AI/Backend Engineer at a **Digital Twin & warehouse automation** startup (VisionSpace)
- â˜ï¸ Shipping real services on **AWS & GCP** with Docker, CI/CD, and observability in mind

---

## ðŸ§  Korean LLM Lineup

### 1. **Qwen2.5-14B-Korean** (Instruction-Tuned) â­
> Korean-focused 14B instruction model based on Qwen2.5-14B-Instruct

[![Model on ðŸ¤—](https://img.shields.io/badge/ðŸ¤—_Qwen2.5--14B--Korean-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/MyeongHo0621/Qwen2.5-14B-Korean)

- Fine-tuned on ~680k Korean instruction / multi-turn / math & reading comprehension samples
- **Benchmarks (Korean):**
  - GSM8K-Ko: 83.5% (167/200)
  - KorQuAD 1.0: 74.2% (371/500)
- Optimized for **long-context Korean reasoning** and multi-turn dialogue

---

### 2. **vclm-korean-7b** â­
> 7B Korean LLM with heavily customized chat format & tokenizer behavior

[![Model on ðŸ¤—](https://img.shields.io/badge/ðŸ¤—_vclm--korean--7b-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/soka0000/vclm-korean-7b)

- Designed as a **playground model** for experimenting with chat templates & instruction styles

---

### 3. **Qwen2.5-3B-Korean (WIP)** ðŸ”§
> Lightweight Korean multi-turn assistant for local & edge deployment

- Base: `Qwen/Qwen2.5-3B-Instruct`
- Goal: Instruction tuning on **SmolTalk-style Korean multi-turn data (~460k)**  
  to build a 3B model that runs on **consumer GPUs (e.g., 6GB VRAM + GGUF)**.
- Plan:
  - LoRA-based SFT (v0) â†’ nf4 / GGUF quantization â†’ `llama.cpp` / Ollama support

---

## ðŸš€ Featured Projects

### 1. **Industrial Digital Twin & RAG Platform** â­
> Backend systems for warehouse automation with AGV/AMR/CNV/RTV integration

**Impact:**
- âš¡ **75% latency reduction** in AI inference (20s â†’ 5s) via vLLM + RAG optimization
- ðŸ—„ï¸ Built a **document RAG** pipeline for real-time equipment / SOP retrieval
- ðŸ“Š XGBoost-based ROI/KPI prediction within simulation environments

**Tech:** Python, FastAPI, Next.js 15, vLLM, ROS, PostgreSQL, AWS EC2, Docker

**Status:** ðŸŸ¢ Production (Aug 2025 â€“ Present)

---

### 2. **Multi-LLM Document Automation System** â­
> Enterprise proposal generation system with Claude, Gemini, GPT, EXAONE integration

**Impact:**
- ðŸ“ **80% reduction** in proposal creation time for exhibition & B2B clients
- ðŸ¤– Multi-LLM orchestration (routing + fallback) with real-time monitoring
- ðŸ“‹ Template-based generation while preserving brand tone & structure

**Tech:** FastAPI, LangChain, Next.js, Docker, AWS EC2

**Status:** ðŸŸ¢ Production

---

### 3. **SOLAR-10.7B Korean Fine-tuning** â­
> MLOps pipeline for Korean language model optimization on H100E

[![Model on ðŸ¤—](https://img.shields.io/badge/ðŸ¤—_eeve--vss--smh-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/MyeongHo0621/eeve-vss-smh)

**Impact:**
- End-to-end MLOps pipeline for **distributed training, logging, and hyperparameter search**
- Published fine-tuned model on HuggingFace for community usage
- Hands-on experience with **H100E GPU** cluster configuration

**Tech:** PyTorch, HuggingFace Transformers, CUDA, H100E

**Status:** ðŸŸ¢ Active Research

---

## ðŸ’» Tech Stack

**LLM & NLP:** Qwen2.5, Llama, SOLAR, EXAONE, vLLM, HuggingFace, RAG, LangChain  
**Training:** LoRA / QLoRA, H100E, bitsandbytes, Unsloth, CUDA  
**Backend:** Python, FastAPI, PostgreSQL, Redis, WebSocket  
**Frontend:** Next.js 15, React, TailwindCSS  
**Cloud & DevOps:** AWS (EC2, ECS, ECR), GCP (Compute Engine), Docker, GitHub Actions

---

## ðŸ“Š Impact Summary

| Metric              | Value          | Context                          |
|---------------------|----------------|----------------------------------|
| Inference Latency   | **-75%**       | 20s â†’ 5s via vLLM optimization   |
| Document Time Saved | **-80%**       | Proposal & doc automation        |
| Production Systems  | **3+**         | Multi-cloud, customer-facing     |
| Public LLMs         | **3+**         | Korean-focused models on HF      |

---

## ðŸ’¼ Experience

**AI/Backend Engineer** @ VisionSpace (Digital Twin & Automation)  
*Aug 2025 â€“ Present*

**AI Product / Backend Engineer** @ Nuckl  
*Jan 2025 â€“ Feb 2025*

**AI Developer** @ GIWorks (Exhibition & LED Production)  
*Jul 2024 â€“ Dec 2024*

---

<div align="center">

![](https://komarev.com/ghpvc/?username=EnzoMH&color=blue&style=flat-square)

**"From Korean LLM research to production RAG systems."**

</div>
