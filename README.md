<div align="center">

# Ïã†Î™ÖÌò∏ Shin Myeongho

**AI/ML Engineer** | Production Systems & LLM Optimization

[![HuggingFace](https://img.shields.io/badge/ü§ó_MyeongHo0621-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/MyeongHo0621)
[![HuggingFace](https://img.shields.io/badge/ü§ó_soka0000-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/soka0000)
[![Email](https://img.shields.io/badge/isfs003@gmail.com-EA4335?style=flat-square&logo=gmail&logoColor=white)](mailto:isfs003@gmail.com)
[![Portfolio](https://img.shields.io/badge/Portfolio-000000?style=flat-square&logo=notion&logoColor=white)](https://www.notion.so/Shin-Myeong-Ho-32b17c808b3642a583ea457a0c68df5a)

</div>

---

## üëã About Me

Building production AI systems with focus on **LLM optimization** and **cloud infrastructure**.

- ü§ñ Reduced AI inference latency by **75%** (20s ‚Üí 5s) using vLLM + RAG optimization
- üè≠ Currently: Backend Engineer at Digital Twin platform (warehouse automation)
- üî¨ Research: Fine-tuning SOLAR-10.7B for Korean language on H100E GPU
- ‚òÅÔ∏è Multi-cloud: AWS (EC2, ECS, ECR) & GCP production deployments

---

## üöÄ Featured Projects

### 1. **Industrial Digital Twin Platform** ‚≠ê
> Backend systems for warehouse automation with AGV/AMR/CNV/RTV integration

**Impact:**
- ‚ö° **75% latency reduction** in AI inference (20s ‚Üí 5s) via vLLM optimization
- üìä XGBoost-based ROI/KPI prediction in simulation environments
- üóÑÔ∏è RAG system with vector databases for real-time data retrieval

**Tech:** Python, FastAPI, Next.js 15, vLLM, ROS, AWS EC2

**Status:** üü¢ Production (Aug 2025 - Present)

---

### 2. **SOLAR-10.7B Korean Fine-tuning** ‚≠ê
> MLOps pipeline for Korean language model optimization

[![Model on ü§ó](https://img.shields.io/badge/ü§ó_eeve--vss--smh-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/MyeongHo0621/eeve-vss-smh)

**Impact:**
- üß† Complete MLOps pipeline: distributed training, hyperparameter tuning, monitoring
- üéØ Published model on HuggingFace for community use
- ‚öôÔ∏è H100E GPU infrastructure configuration

**Tech:** PyTorch, HuggingFace Transformers, CUDA, H100E

**Status:** üü¢ Active Research (Sep 2025 - Present)

---

### 3. **Multi-LLM Document Automation** ‚≠ê
> Enterprise proposal generation system with Claude, Gemini, GPT integration

**Impact:**
- üìù **80% reduction** in proposal creation time
- ü§ñ Multi-LLM orchestration with real-time monitoring
- üìã Template-based generation maintaining quality consistency

**Tech:** FastAPI, LangChain, Next.js, Docker, AWS EC2

**Status:** üü¢ Production (AWS)

---

## üíª Tech Stack

**AI/ML:** vLLM, LangChain, HuggingFace, PyTorch, XGBoost, RAG Systems

**Backend:** Python, FastAPI, PostgreSQL, ROS

**Frontend:** Next.js 15, React, TailwindCSS

**Cloud:** AWS (EC2, ECS, ECR), GCP (Compute Engine), Docker, GitHub Actions

---

## üìä Impact Summary

| Metric | Value | Context |
|--------|-------|---------|
| Latency Reduction | **75%** | 20s ‚Üí 5s via vLLM |
| Time Saved | **80%** | Document automation |
| Production Systems | **3** | Multi-cloud deployments |
| LLM Integrations | **6** | Claude, GPT, Gemini, SOLAR, Llama, EXAONE |

---

## üíº Experience

**Backend Systems Engineer** @ VisionSpace (Digital Twin)  
*Aug 2025 - Present*

**AI Product Manager** @ Nuckl  
*Jan 2025 - Feb 2025*

**AI Developer** @ GIWorks  
*Jul 2024 - Dec 2024*

---

<div align="center">

![](https://komarev.com/ghpvc/?username=EnzoMH&color=blue&style=flat-square)

**"Optimizing AI systems from research to production"**

</div>
